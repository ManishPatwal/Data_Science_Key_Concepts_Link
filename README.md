# Data_Science_Key_Concepts_Link
This repo contains link of Important Data Science Articles, Concepts and Approaches from around the web.
**Glossary ML &DS **
https://www.analyticsvidhya.com/glossary-of-common-statistics-and-machine-learning-terms/

**Machine Learning**
https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/

**Suport Vector Machine**
https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/

If you have a classification problem, i.e., discrete label to predict, you can use C-classification and nu-classification.

If you have a regression problem, i.e., continuous number to predict, you can use eps-regression and nu-regression.

If you only have one class of the data, i.e., normal behavior, and want to detect outliers.  one-classification.

**Tree Based Modelling from Scratch**
https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/

**Tidy Text Data in R**
https://www.tidytextmining.com/preface.html
https://www.youtube.com/watch?v=0poJP8WQxew

**Creating Packages**
Caret Package in R

https://www.analyticsvidhya.com/blog/2016/12/practical-guide-to-implement-machine-learning-with-caret-package-in-r-with-practice-problem/https://www.analyticsvidhya.com/blog/2016/12/practical-guide-to-implement-machine-learning-with-caret-package-in-r-with-practice-problem/

**Shiny App Tutorial**
https://youtu.be/_0ORRJqctHE
https://www.analyticsvidhya.com/blog/2016/10/creating-interactive-data-visualization-using-shiny-app-in-r-with-examples/

LEAFLET & SHINY INTERACTIVE TUTORIAL
https://www.datascience.com/blog/beginners-guide-to-shiny-and-leaflet-for-interactive-mapping

##MATHS

How do data scientists use statistics? by William Chen https://www.quora.com/How-do-data-scientists-use-statistics/answer/William-Chen-6?share=7310896c&srid=nLpS


What is the best way to learn basic statistics besides taking a college course? by Alexey Grigorev https://www.quora.com/What-is-the-best-way-to-learn-basic-statistics-besides-taking-a-college-course/answer/Alexey-Grigorev?share=f8b0af74&srid=nLpS


Linear algebra
https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/

Linear Regression/Lasso Regression /Ridge 
https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/

##Other Key Concepts
CPU vs GPU
https://www.analyticsvidhya.com/blog/2017/05/gpus-necessary-for-deep-learning/

**Deep Learning **

Neural network

https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/

Classification Methods Essentials
http://www.sthda.com/english/articles/36-classification-methods-essentials/149-penalized-logistic-regression-essentials-in-r-ridge-lasso-and-elastic-net/

Mathematics
information theory
https://en.wikipedia.org/wiki/Entropy_(information_theory)


Penalized Logistic Regression Essentials in R: Ridge, Lasso and Elastic Net

Penalized logistic regression imposes a penalty to the logistic model for having too many variables. This results in shrinking the coefficients of the less contributive variables toward zero. This is also known as regularization.

The most commonly used penalized regression include:

ridge regression: variables with minor contribution have their coefficients close to zero. However, all the variables are incorporated in the model. This is useful when all variables need to be incorporated in the model according to domain knowledge.
lasso regression: the coefficients of some less contributive variables are forced to be exactly zero. Only the most significant variables are kept in the final model.
elastic net regression: the combination of ridge and lasso regression. It shrinks some coefficients toward zero (like ridge regression) and set some coefficients to exactly zero (like lasso regression)
https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a

Bagging vs Bootstrapping
The big difference between bagging and validation techniques is that bagging averages models (or predictions of an ensemble of models) in order to reduce the variance the prediction is subject to while resampling validation such as cross validation and out-of-bootstrap validation evaluate a number of surrogate models assuming that they are equivalent (i.e. a good surrogate) for the actual model in question which is trained on the whole data set.

Bagging uses bootstrapped subsets (i.e. drawing with replacement of the original data set) of training data to generate such an ensemble but you can also use ensembles that are produced by drawing without replacement, i.e. cross validation: 


http://www.robots.ox.ac.uk/~az/lectures/ml/lect4.pdf

**Bias Vs Variance**
https://towardsdatascience.com/balancing-bias-and-variance-to-control-errors-in-machine-learning-16ced95724db

**WEB APIS in R**
https://www.youtube.com/watch?v=lV3u6Dc93T0


